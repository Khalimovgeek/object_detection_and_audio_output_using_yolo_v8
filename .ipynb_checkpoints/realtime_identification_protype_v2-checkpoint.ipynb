{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004f325d-e870-4f70-90cc-2328ecf0e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pyttsx3\n",
    "\n",
    "language='en'\n",
    "# Initialize YOLOv8 model\n",
    "model = YOLO('yolov8m_saved_model/yolov8m_float32.tflite')\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "                  \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "                  \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "                  \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "                  \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "                  \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "                  \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "                  \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "                  \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "                  \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "                  ]\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b23a32-b426-46de-bb1c-115b3906eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio(path):\n",
    "    \n",
    "    language = 'en'\n",
    "    results = model(path)\n",
    "    x = \"There \"  # Initialize x\n",
    "    for result in results:\n",
    "        boxes =result.boxes.cls\n",
    "        boxes_int = [int(x) for x in boxes]\n",
    "        counter = Counter(boxes_int)\n",
    "\n",
    "        print(counter)  # Check what boxes contains\n",
    "        for i, (box, count) in enumerate(counter.items()):\n",
    "            if i != len(counter) - 1 and i != 0:\n",
    "                if count == 1:\n",
    "                    x += f\"a {classNames[int(box)]} , \"\n",
    "                else:\n",
    "                    x += f\"{count} {classNames[int(box)]}s , \"\n",
    "            elif i == 0:\n",
    "                if count == 1:\n",
    "                    x += f\"is a {classNames[int(box)]} , \"\n",
    "                else:\n",
    "                    x += f\"are {count} {classNames[int(box)]}s , \"\n",
    "            else:\n",
    "                if count == 1:\n",
    "                    x += f\"and a {classNames[int(box)]} \"\n",
    "                else:\n",
    "                    x += f\"and {count} {classNames[int(box)]}s \"\n",
    "    \n",
    "    x+= \"in front of you.\"\n",
    "    print(x)\n",
    "    engine.say(x)\n",
    "    engine.runAndWait()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67027dfc-2b13-49f7-9266-92664f9be616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_and_speak(frame):\n",
    "    # Convert frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run YOLOv8 tracking\n",
    "    results = model.track(frame, persist=True)\n",
    "    boxes = results[0].boxes.xywh.cpu()\n",
    "    #probs = results.probs.cpu()\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        print(\"No moving objects detected.\")\n",
    "    else:\n",
    "        objects_with_direction = []  # List to store objects along with their direction\n",
    "        objects=[]\n",
    "        for j,box in enumerate(boxes):\n",
    "            # Extract box coordinates\n",
    "            x, y, w, h = box\n",
    "            # Calculate box center\n",
    "            center_x =[]\n",
    "            center_x.append(x)\n",
    "            for xx in center_x:\n",
    "            \n",
    "                print(xx)\n",
    "                # Determine direction based on box center\n",
    "                if xx <= 215:\n",
    "                    direction = \"left\"\n",
    "                elif xx >= 430:\n",
    "                    direction = \"right\"\n",
    "                else:\n",
    "                    direction = \"middle\"\n",
    "                print(direction)\n",
    "                object_name = classNames[int(results[0].boxes.cls[j])]\n",
    "                print(object_name)\n",
    "                objects_with_direction.append(f\" {direction}\")\n",
    "                objects.append(f\" {object_name}\")\n",
    "                text_to_speak = f\"The {objects} is in your {objects_with_direction}\" \n",
    "        \n",
    "        \n",
    "                engine.say(text_to_speak)\n",
    "                engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a601f6-298a-4f98-95de-c45028ad9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_distance(frame):\n",
    "        # Convert frame to grayscale\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run YOLOv8 tracking\n",
    "        results = model.track(frame, persist=True)\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        #probs = results.probs.cpu()\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "                print(\"No moving objects detected.\")\n",
    "        else:\n",
    "            objects_with_direction = []  # List to store objects along with their direction\n",
    "            objects=[]\n",
    "            for j,box in enumerate(boxes):\n",
    "                        # Extract box coordinates\n",
    "                        x, y, w, h = box\n",
    "                        # Calculate box center\n",
    "                        print(box)\n",
    "\n",
    "                        focal_length = 1.479685039370079  # Focal length of the camera\n",
    "                        object_height = 0.2  # Height of the object in meters\n",
    "                        image_height = 1080  # Image height in pixels\n",
    "                        object_pixel_height=h\n",
    "                        distance = (focal_length * object_height * image_height) / (object_pixel_height )\n",
    "                        print( f'Distance: {distance:.2f} meters')\n",
    "                        dist=f\"The Object is in {distance:.2f} meters\"\n",
    "                        engine.say(dist)\n",
    "                        engine.runAndWait()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445b8ed0-87b3-45fc-88bd-7aceb5004e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/user/myenv/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolov8m_saved_model/yolov8m_float32.tflite for TensorFlow Lite inference...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/user/object_detection_and_audio_output_using_yolo_v8/temp_img.jpg: 640x640 3 persons, 1 chair, 3 laptops, 2577.7ms\n",
      "Speed: 8.1ms preprocess, 2577.7ms inference, 933.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Counter({0: 3, 63: 3, 56: 1})\n",
      "There is a chair , 3 persons , and 3 laptops in front of you.\n",
      "\n",
      "0: 640x640 1 person, 1 chair, 2627.7ms\n",
      "Speed: 5.3ms preprocess, 2627.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor([322.6153, 207.5734,  83.3015, 122.4331])\n",
      "Distance: 2.61 meters\n",
      "tensor([398.7616, 173.4089,  52.6813, 161.7538])\n",
      "Distance: 1.98 meters\n",
      "\n",
      "0: 640x640 1 person, 1 chair, 2556.9ms\n",
      "Speed: 5.4ms preprocess, 2556.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor(296.2806)\n",
      "middle\n",
      "chair\n",
      "tensor(362.7044)\n",
      "middle\n",
      "person\n",
      "\n",
      "image 1/1 /home/user/object_detection_and_audio_output_using_yolo_v8/temp_img.jpg: 640x640 1 chair, 2537.9ms\n",
      "Speed: 6.4ms preprocess, 2537.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Counter({56: 1})\n",
      "There is a chair , in front of you.\n",
      "\n",
      "0: 640x640 1 person, 1 chair, 2699.8ms\n",
      "Speed: 6.3ms preprocess, 2699.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor([239.8002, 184.0034,  83.1385, 124.5023])\n",
      "Distance: 2.57 meters\n",
      "tensor([535.5239, 173.6612,  78.0910, 255.9858])\n",
      "Distance: 1.25 meters\n",
      "\n",
      "0: 640x640 1 person, 1 chair, 1 laptop, 2673.7ms\n",
      "Speed: 8.0ms preprocess, 2673.7ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor(227.8254)\n",
      "middle\n",
      "chair\n",
      "tensor(522.4881)\n",
      "right\n",
      "person\n",
      "tensor(600.7722)\n",
      "right\n",
      "laptop\n",
      "\n",
      "image 1/1 /home/user/object_detection_and_audio_output_using_yolo_v8/temp_img.jpg: 640x640 1 chair, 2762.1ms\n",
      "Speed: 6.3ms preprocess, 2762.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Counter({56: 1})\n",
      "There is a chair , in front of you.\n",
      "\n",
      "0: 640x640 1 chair, 2556.2ms\n",
      "Speed: 7.7ms preprocess, 2556.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor([295.4174, 197.2195,  81.8354, 120.5533])\n",
      "Distance: 2.65 meters\n",
      "\n",
      "0: 640x640 1 person, 1 chair, 2570.7ms\n",
      "Speed: 4.6ms preprocess, 2570.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor(299.6420)\n",
      "middle\n",
      "chair\n",
      "tensor(191.4454)\n",
      "left\n",
      "person\n",
      "\n",
      "image 1/1 /home/user/object_detection_and_audio_output_using_yolo_v8/temp_img.jpg: 640x640 1 person, 1 chair, 2568.6ms\n",
      "Speed: 5.6ms preprocess, 2568.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Counter({56: 1, 0: 1})\n",
      "There is a chair , and a person in front of you.\n",
      "\n",
      "0: 640x640 4 persons, 1 chair, 2668.8ms\n",
      "Speed: 3.9ms preprocess, 2668.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor([199.5290, 220.4438,  61.3649, 114.1666])\n",
      "Distance: 2.80 meters\n",
      "tensor([137.1437, 182.8543,  84.9732, 319.3170])\n",
      "Distance: 1.00 meters\n",
      "tensor([569.1982, 214.7427, 100.4870, 284.9410])\n",
      "Distance: 1.12 meters\n",
      "tensor([628.1262, 207.2554,  22.6910,  58.8809])\n",
      "Distance: 5.43 meters\n",
      "tensor([519.6249, 208.2548,  36.1767,  49.8676])\n",
      "Distance: 6.41 meters\n",
      "\n",
      "0: 640x640 4 persons, 1 chair, 2677.9ms\n",
      "Speed: 5.2ms preprocess, 2677.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor(208.6961)\n",
      "left\n",
      "chair\n",
      "tensor(156.5623)\n",
      "left\n",
      "person\n",
      "tensor(574.4738)\n",
      "right\n",
      "person\n",
      "tensor(528.2357)\n",
      "right\n",
      "person\n",
      "tensor(410.7779)\n",
      "middle\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gpiozero import Button\n",
    "button_pin = 2  # Example pin for the button\n",
    "button = Button(button_pin)\n",
    "def function1(img):\n",
    "    cv2.imwrite(\"temp_img.jpg\", img)  # Save a temporary image\n",
    "    audio(\"temp_img.jpg\")\n",
    "def function2(frame):\n",
    "    detect_distance(frame)\n",
    "def function3(frame):\n",
    "    detect_objects_and_speak(frame)\n",
    "functions = [function1, function2, function3]\n",
    "current_function_index = 0\n",
    "# Main function to capture frames and handle events\n",
    "def detect():\n",
    "    # Replace \"http://192.168.29.111:8080\" with the URL of your IP webcam stream\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Set the resolution (optional)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1080)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1920)\n",
    "    \n",
    "    # Check if the video stream opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open video stream\")\n",
    "        return\n",
    "    current_function_index = 0\n",
    "    # Main loop\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly ret is True\n",
    "        if not ret:\n",
    "            print(\"Error: Couldn't read frame\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        \n",
    "        if button.is_pressed:\n",
    "            # Call the current function and increment the index\n",
    "            functions[current_function_index](frame)\n",
    "            current_function_index = (current_function_index + 1) % len(functions)\n",
    "\n",
    "        # Check for key events\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        # Perform actions based on key pressed\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            break  # Break the loop if 'q' is pressed\n",
    "\n",
    "    # Release the video stream and close OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the detect function\n",
    "detect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac2ac0-f51f-43fd-aef4-02e001a8f075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242ecce-786f-495a-ba6f-a2a393fb816a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
