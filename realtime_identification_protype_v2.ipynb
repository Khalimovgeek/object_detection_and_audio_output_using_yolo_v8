{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004f325d-e870-4f70-90cc-2328ecf0e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "model = YOLO('yolov8m_saved_model/yolov8m_float32.tflite')\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "                  \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "                  \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "                  \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "                  \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "                  \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "                  \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "                  \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "                  \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "                  \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "                  ]\n",
    "# Initialize pyttsx3 engine\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b23a32-b426-46de-bb1c-115b3906eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio(path):\n",
    "    \n",
    "    language = 'en'\n",
    "    results = model(path)\n",
    "    x = \"There \"  # Initialize x\n",
    "    for result in results:\n",
    "        boxes =result.boxes.cls\n",
    "        boxes_int = [int(x) for x in boxes]\n",
    "        counter = Counter(boxes_int)\n",
    "\n",
    "        print(counter)  # Check what boxes contains\n",
    "        for i, (box, count) in enumerate(counter.items()):\n",
    "            if i != len(counter) - 1 and i != 0:\n",
    "                if count == 1:\n",
    "                    x += f\"a {classNames[int(box)]} , \"\n",
    "                else:\n",
    "                    x += f\"{count} {classNames[int(box)]}s , \"\n",
    "            elif i == 0:\n",
    "                if count == 1:\n",
    "                    x += f\"is a {classNames[int(box)]} , \"\n",
    "                else:\n",
    "                    x += f\"are {count} {classNames[int(box)]}s , \"\n",
    "            else:\n",
    "                if count == 1:\n",
    "                    x += f\"and a {classNames[int(box)]} \"\n",
    "                else:\n",
    "                    x += f\"and {count} {classNames[int(box)]}s \"\n",
    "    \n",
    "    x+= \"in front of you.\"\n",
    "    print(x)\n",
    "    pyttsx3.speak(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5e12368-5f86-4992-9928-b4b2168e0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform object detection and speak the detected objects and their directions\n",
    "def detect_objects_and_speak(frame):\n",
    "    # Convert frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Run YOLOv8 tracking\n",
    "    results = model.track(frame, persist=True)\n",
    "    boxes = results[0].boxes.xywh.cpu()\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        print(\"No moving objects detected.\")\n",
    "    else:\n",
    "               # Calculate the center of the frame\n",
    "        frame_center_x = frame.shape[1] // 2\n",
    "        text_to_speak = \"There is a\"\n",
    "        \n",
    "        objects_with_direction = []  # List to store objects along with their direction\n",
    "        \n",
    "        for i, obj in enumerate(boxes):\n",
    "            object_center_x1 = obj[0] + obj[2] / 2\n",
    "            \n",
    "            # Calculate the distance between the object's center and the frame center\n",
    "            distance_from_center = abs(object_center_x1 - frame_center_x)\n",
    "            \n",
    "            if distance_from_center > 50:  # Adjust the threshold as needed\n",
    "                direction = \"on the left\" if object_center_x1 < frame_center_x else \"on the right\"\n",
    "            else:\n",
    "                direction = \"in the middle\"\n",
    "        \n",
    "            object_name = classNames[int(results[0].boxes.cls[i])]\n",
    "            \n",
    "            # Add object along with its direction to the list\n",
    "            objects_with_direction.append(f\" {object_name} {direction}\")\n",
    "        \n",
    "        # Join the objects with their directions into a single string\n",
    "        text_to_speak += \", \".join(objects_with_direction[:-1])  # Add objects with direction except the last one\n",
    "        if len(objects_with_direction) > 1:  # Add 'and' if there are multiple objects\n",
    "            text_to_speak += \", and \" + objects_with_direction[-1]\n",
    "        else:\n",
    "            text_to_speak += objects_with_direction[-1]\n",
    "        \n",
    "        # Speak out the detected objects and their directions\n",
    "        engine.say(text_to_speak)\n",
    "        engine.runAndWait()\n",
    "        \n",
    "        # Print the detected objects and their directions\n",
    "        print(text_to_speak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "445b8ed0-87b3-45fc-88bd-7aceb5004e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolov8m_saved_model\\yolov8m_float32.tflite for TensorFlow Lite inference...\n",
      "\n",
      "image 1/1 C:\\Users\\User\\temp_img.jpg: 640x640 1 person, 1 bed, 2 refrigerators, 1080.6ms\n",
      "Speed: 5.0ms preprocess, 1080.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Counter({72: 2, 0: 1, 59: 1})\n",
      "There is a person , 2 refrigerators , and a bed in front of you.\n",
      "\n",
      "0: 640x640 1 person, 949.6ms\n",
      "Speed: 4.0ms preprocess, 949.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "There is a person on the right\n",
      "\n",
      "0: 640x640 1 person, 962.1ms\n",
      "Speed: 3.0ms preprocess, 962.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "There is a person on the right\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 964.1ms\n",
      "Speed: 4.0ms preprocess, 964.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "There is a person on the right, and  refrigerator on the left\n"
     ]
    }
   ],
   "source": [
    "# Main function to capture frames and handle events\n",
    "def detect():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 1920)\n",
    "    cap.set(4, 1080)\n",
    "    \n",
    "    # Main loop\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        \n",
    "        # Perform object detection and speak the detected objects and their directions when 'u' is pressed\n",
    "        if key == ord('u'):\n",
    "            detect_objects_and_speak(frame)  # Call the audio function with the path to the temporary image\n",
    "        elif key == ord('r'):\n",
    "            cv2.imwrite(\"temp_img.jpg\", frame)  # Save a temporary image\n",
    "            audio(\"temp_img.jpg\")  # Call the audio function with the path to the temporary image\n",
    "        # Break the loop if 'q' is pressed\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the detect function\n",
    "detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67027dfc-2b13-49f7-9266-92664f9be616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
